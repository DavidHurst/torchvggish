{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define test files\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import gzip\n",
    "import random\n",
    "import vggish_input\n",
    "import vggish_postprocess\n",
    "import vggish_params\n",
    "import vggish_slim\n",
    "import tensorflow as tf\n",
    "import torchvggish\n",
    "import torch \n",
    "\n",
    "\n",
    "def example_generator(base_path):\n",
    "    \"\"\"\n",
    "    Emits random examples from partition of audioset\n",
    "    \"\"\"\n",
    "    eval_files = os.listdir(base_path)\n",
    "    random.shuffle(eval_files)\n",
    "    while True:\n",
    "        eg_file = base_path + eval_files.pop(0)\n",
    "        with gzip.open(eg_file, 'rb') as wav_file:\n",
    "            yield vggish_input.wavfile_to_examples(wav_file)\n",
    "            \n",
    "eval_path = \"../../data/audioset_eval/\"\n",
    "data_generator = example_generator(eval_path)\n",
    "pproc = vggish_postprocess.Postprocessor(\"vggish_pca_params.npz\")\n",
    "examples_batch = next(data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "[[186  29 203 ... 164   0 255]\n",
      " [187  31 180 ... 128   0 255]\n",
      " [192  40 209 ... 111   0 255]\n",
      " ...\n",
      " [188  36 203 ...   0   0 255]\n",
      " [180  22 173 ...   0   0 255]\n",
      " [182  27 190 ...   0  47 255]]\n"
     ]
    }
   ],
   "source": [
    "# Create an embedding with tensorflow vggish\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    vggish_slim.define_vggish_slim(training=False)\n",
    "    vggish_slim.load_vggish_slim_checkpoint(sess, \"vggish_model.ckpt\")\n",
    "    features_tensor = sess.graph.get_tensor_by_name(\n",
    "        vggish_params.INPUT_TENSOR_NAME)\n",
    "    embedding_tensor = sess.graph.get_tensor_by_name(\n",
    "        vggish_params.OUTPUT_TENSOR_NAME)\n",
    "\n",
    "    # Run inference and postprocessing.\n",
    "    [tf_embedding_batch] = sess.run([embedding_tensor],\n",
    "                                 feed_dict={features_tensor: examples_batch})\n",
    "    tf_postprocessed_batch = pproc.postprocess(tf_embedding_batch)\n",
    "    print(tf_postprocessed_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/harrisontaylor/Workspace/Research/audio-experiments/torchvggish\r\n",
      "[[244   0 158 ... 204 255 255]\n",
      " [237   0 160 ... 219 255 255]\n",
      " [233   0 157 ...  78 255 255]\n",
      " ...\n",
      " [233   0 161 ... 244 255 255]\n",
      " [234   0 155 ... 255 255 255]\n",
      " [240   0 155 ... 255 255 255]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, ..., False, False,  True],\n",
       "       [False, False, False, ..., False, False,  True],\n",
       "       [False, False, False, ..., False, False,  True],\n",
       "       ...,\n",
       "       [False, False, False, ..., False, False,  True],\n",
       "       [False, False, False, ..., False, False,  True],\n",
       "       [False, False, False, ..., False, False,  True]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create an embedding with pytorch vggish\n",
    "pt_examples_batch = examples_batch[:,None,:,:]\n",
    "pt_examples_batch = torch.from_numpy(pt_examples_batch).float()\n",
    "\n",
    "!pwd\n",
    "net = torchvggish.VGGish()\n",
    "net.load_state_dict(torch.load(\"vggish-model.pth\"))\n",
    "pt_embedding_batch = net.forward(pt_examples_batch)\n",
    "pt_embedding_batch = pt_embedding_batch.data.numpy()\n",
    "\n",
    "pt_postprocessed_batch = pproc.postprocess(pt_embedding_batch)\n",
    "print(pt_postprocessed_batch)\n",
    "pt_postprocessed_batch == tf_postprocessed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Shit. Okay, time to debug...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Keras Version :-) \n",
    "\n",
    "\"\"\"VGGish model for Keras. A VGG-like model for audio classification\n",
    "# Reference\n",
    "- [CNN Architectures for Large-Scale Audio Classification](ICASSP 2017)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D, GlobalMaxPooling2D\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "# weight path\n",
    "WEIGHTS_PATH = './keras_weights.h5'\n",
    "\n",
    "def KerasVGGish(load_weights=True, weights='audioset',\n",
    "           input_tensor=None, input_shape=None,\n",
    "           out_dim=None, pooling='avg'):\n",
    "    '''\n",
    "    An implementation of the VGGish architecture.\n",
    "    :param load_weights: if load weights\n",
    "    :param weights: loads weights pre-trained on a preliminary version of YouTube-8M.\n",
    "    :param input_tensor: input_layer\n",
    "    :param input_shape: input data shape\n",
    "    :param out_dim: output dimension\n",
    "    :param include_top:whether to include the 3 fully-connected layers at the top of the network.\n",
    "    :param pooling: pooling type over the non-top network, 'avg' or 'max'\n",
    "    :return: A Keras model instance.\n",
    "    '''\n",
    "\n",
    "    if weights not in {'audioset', None}:\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization) or `audioset` '\n",
    "                         '(pre-training on audioset).')\n",
    "\n",
    "    if out_dim is None:\n",
    "        out_dim = vggish_params.EMBEDDING_SIZE\n",
    "\n",
    "    # input shape\n",
    "    if input_shape is None:\n",
    "        input_shape = (vggish_params.NUM_FRAMES, vggish_params.NUM_BANDS, 1)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        aud_input = Input(shape=input_shape, name='input_1')\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            aud_input = Input(tensor=input_tensor, shape=input_shape, name='input_1')\n",
    "        else:\n",
    "            aud_input = input_tensor\n",
    "\n",
    "\n",
    "    # Block 1\n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same', name='features.0')(aud_input)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='features.2')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same', name='features.3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='features.5')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='same', name='features.6')(x)\n",
    "    x = Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='same', name='features.8')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='features.10')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), strides=(1, 1), activation='relu', padding='same', name='features.11')(x)\n",
    "    x = Conv2D(512, (3, 3), strides=(1, 1), activation='relu', padding='same', name='features.13')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='features.15')(x)\n",
    "    \n",
    "    x = Flatten(name='flatten_')(x)\n",
    "    x = Dense(4096, activation='relu', name='embeddings.0')(x)\n",
    "    x = Dense(4096, activation='relu', name='embeddings.2')(x)\n",
    "    x = Dense(out_dim, activation='relu', name='embeddings.4')(x)\n",
    "\n",
    "\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = aud_input\n",
    "    # Create model.\n",
    "    model = Model(inputs, x, name='VGGish')\n",
    "\n",
    "\n",
    "    # load weights\n",
    "    model.load_weights(WEIGHTS_PATH)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keras_net = KerasVGGish()\n",
    "# k_embedding_batch = keras_net.predict(examples_batch[:,:,:,None])\n",
    "# k_postprocessed_batch = pproc.postprocess(k_embedding_batch)\n",
    "# k_postprocessed_batch == tf_postprocessed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Users/harrisontaylor/.conda/envs/audioset-experiments/lib/python3.6/site-packages/torch/_C.cpython-36m-darwin.so, 9): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n  Referenced from: /Users/harrisontaylor/.conda/envs/audioset-experiments/lib/python3.6/site-packages/torch/lib/libshm.dylib\n  Reason: image not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-abdef1f82af5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Try out the nn-transfer library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvggish\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVGGish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/Research/audio-experiments/torchvggish/torchvggish.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvggish_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/audioset-experiments/lib/python3.6/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0m_dl_flags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m __all__ += [name for name in dir(_C)\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/Users/harrisontaylor/.conda/envs/audioset-experiments/lib/python3.6/site-packages/torch/_C.cpython-36m-darwin.so, 9): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n  Referenced from: /Users/harrisontaylor/.conda/envs/audioset-experiments/lib/python3.6/site-packages/torch/lib/libshm.dylib\n  Reason: image not found"
     ]
    }
   ],
   "source": [
    "#Try out the nn-transfer library\n",
    "\n",
    "from torchvggish import VGGish\n",
    "import torch\n",
    "\n",
    "pytorch_network = VGGish()\n",
    "print(pytorch_network)\n",
    "pytorch_network.save(\"torch_net.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "keras_network = KerasVGGish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer names in PyTorch state_dict ['features.0', 'features.3', 'features.6', 'features.8', 'features.11', 'features.13', 'embeddings.0', 'embeddings.2', 'embeddings.4']\n",
      "Layer names in Keras HDF5 ['embeddings.0', 'embeddings.2', 'embeddings.4', 'features.0', 'features.10', 'features.11', 'features.13', 'features.15', 'features.2', 'features.3', 'features.5', 'features.6', 'features.8', 'flatten_', 'input_1']\n"
     ]
    }
   ],
   "source": [
    "transfer.keras_to_pytorch(keras_network, pytorch_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[162,  19, 169, ...,   0,  44, 255],\n",
       "       [160,  16, 163, ...,   0, 236, 255],\n",
       "       [161,  20, 166, ...,   0, 116, 255],\n",
       "       ...,\n",
       "       [156,  13, 163, ...,   0, 176, 255],\n",
       "       [152,  13, 156, ...,  20,  30, 255],\n",
       "       [149,   0, 158, ...,   0,  61, 255]], dtype=uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_embedding_batch = keras_network.predict(examples_batch[:,:,:,None])\n",
    "k_postprocessed_batch = pproc.postprocess(k_embedding_batch)\n",
    "k_postprocessed_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[244,   0, 154, ..., 255, 255, 255],\n",
       "       [241,   0, 152, ..., 255, 255, 255],\n",
       "       [247,   0, 147, ..., 255, 255, 255],\n",
       "       ...,\n",
       "       [251,   0, 146, ..., 255, 255, 255],\n",
       "       [242,   0, 151, ..., 255, 255, 255],\n",
       "       [230,   0, 159, ..., 101, 255, 255]], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_examples_batch = examples_batch[:,None,:,:]\n",
    "pt_examples_batch = torch.from_numpy(pt_examples_batch).float()\n",
    "\n",
    "\n",
    "pt_embedding_batch = pytorch_network.forward(pt_examples_batch)\n",
    "pt_embedding_batch = pt_embedding_batch.data.numpy()\n",
    "\n",
    "pt_postprocessed_batch = pproc.postprocess(pt_embedding_batch)\n",
    "pt_postprocessed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 96, 64, 1)         0         \n",
      "_________________________________________________________________\n",
      "features.0 (Conv2D)          (None, 96, 64, 64)        640       \n",
      "_________________________________________________________________\n",
      "features.2 (MaxPooling2D)    (None, 48, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "features.3 (Conv2D)          (None, 48, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "features.5 (MaxPooling2D)    (None, 24, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "features.6 (Conv2D)          (None, 24, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "features.8 (Conv2D)          (None, 24, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "features.10 (MaxPooling2D)   (None, 12, 8, 256)        0         \n",
      "_________________________________________________________________\n",
      "features.11 (Conv2D)         (None, 12, 8, 512)        1180160   \n",
      "_________________________________________________________________\n",
      "features.13 (Conv2D)         (None, 12, 8, 512)        2359808   \n",
      "_________________________________________________________________\n",
      "features.15 (MaxPooling2D)   (None, 6, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_ (Flatten)           (None, 12288)             0         \n",
      "_________________________________________________________________\n",
      "embeddings.0 (Dense)         (None, 4096)              50335744  \n",
      "_________________________________________________________________\n",
      "embeddings.2 (Dense)         (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "embeddings.4 (Dense)         (None, 128)               524416    \n",
      "=================================================================\n",
      "Total params: 72,141,184\n",
      "Trainable params: 72,141,184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "VGGish(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU(inplace)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace)\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace)\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): ReLU(inplace)\n",
      "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (14): ReLU(inplace)\n",
      "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (embeddings): Sequential(\n",
      "    (0): Linear(in_features=12288, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (3): ReLU(inplace)\n",
      "    (4): Linear(in_features=4096, out_features=128, bias=True)\n",
      "    (5): ReLU(inplace)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 7.7917e-03, 5.8261e-03, 0.0000e+00, 1.4602e-02,\n",
       "         1.2129e-02, 0.0000e+00, 9.5376e-03, 0.0000e+00, 0.0000e+00, 5.3788e-03,\n",
       "         0.0000e+00, 8.2478e-03, 6.0739e-03, 0.0000e+00, 1.6649e-02, 3.0965e-04,\n",
       "         1.5750e-02, 0.0000e+00, 4.4081e-03, 5.2477e-03, 0.0000e+00, 1.3824e-02,\n",
       "         6.4700e-03, 8.3422e-03, 0.0000e+00, 0.0000e+00, 5.4149e-03, 0.0000e+00,\n",
       "         4.4764e-03, 3.5309e-03, 9.0604e-03, 0.0000e+00, 8.9312e-03, 0.0000e+00,\n",
       "         0.0000e+00, 1.1059e-02, 0.0000e+00, 9.1706e-03, 1.3428e-02, 4.2193e-03,\n",
       "         1.1189e-02, 0.0000e+00, 4.8923e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         2.4982e-03, 8.8991e-03, 0.0000e+00, 2.7875e-03, 1.5925e-02, 5.8490e-04,\n",
       "         4.3007e-03, 3.8881e-03, 9.0912e-04, 0.0000e+00, 0.0000e+00, 8.1742e-03,\n",
       "         0.0000e+00, 1.2945e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.8607e-03,\n",
       "         1.2168e-02, 0.0000e+00, 0.0000e+00, 1.3794e-02, 1.0272e-02, 1.3082e-02,\n",
       "         3.2369e-05, 8.8510e-03, 1.0556e-02, 0.0000e+00, 0.0000e+00, 1.5604e-02,\n",
       "         1.5089e-02, 1.2526e-02, 0.0000e+00, 2.3616e-02, 1.0680e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 2.0663e-02, 0.0000e+00, 8.3442e-03, 4.2425e-03,\n",
       "         3.2007e-03, 0.0000e+00, 1.7086e-02, 7.8136e-03, 0.0000e+00, 1.3056e-03,\n",
       "         0.0000e+00, 2.2684e-02, 1.1839e-02, 5.3598e-03, 0.0000e+00, 3.1408e-03,\n",
       "         5.9017e-03, 0.0000e+00, 1.6332e-02, 8.4380e-03, 4.4253e-03, 0.0000e+00,\n",
       "         1.2694e-02, 0.0000e+00, 0.0000e+00, 9.3726e-03, 1.2618e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4603e-02, 0.0000e+00, 1.2079e-02,\n",
       "         1.2307e-02, 0.0000e+00, 1.2351e-02, 2.4449e-03, 0.0000e+00, 0.0000e+00,\n",
       "         1.0593e-02, 0.0000e+00]], grad_fn=<ReluBackward1>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(keras_network.summary())\n",
    "print(pytorch_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False, False, False, False, False, False,  True, False,\n",
       "         True,  True, False,  True, False, False, False, False, False,\n",
       "        False,  True, False, False, False, False, False, False,  True,\n",
       "         True, False,  True, False, False, False, False, False, False,\n",
       "         True, False,  True, False, False, False, False, False, False,\n",
       "        False, False, False, False, False,  True, False, False, False,\n",
       "        False, False, False,  True, False, False,  True, False,  True,\n",
       "         True, False, False, False, False,  True, False, False, False,\n",
       "        False, False, False, False,  True, False, False, False, False,\n",
       "        False, False,  True, False, False, False,  True, False, False,\n",
       "        False, False, False, False, False, False,  True, False, False,\n",
       "        False,  True, False, False, False, False, False, False,  True,\n",
       "        False,  True,  True, False, False,  True,  True,  True, False,\n",
       "        False,  True, False, False, False, False, False,  True,  True,\n",
       "        False, False]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.rand(1,1,64,96).float()\n",
    "data_keras = data.numpy().swapaxes(1,3)\n",
    "\n",
    "pt_result = pytorch_network.forward(data)\n",
    "ks_result = keras_network.predict(data_keras)\n",
    "ks_result == pt_result.data.numpy() # false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"vggish_keras.json\", \"w\") as f:\n",
    "    f.write(keras_network.to_json())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/harrisontaylor/.conda/envs/audioset-experiments/lib/python3.7/site-packages/keras2onnx/subgraph.py:140: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.remove_training_nodes\n",
      "using tensorflow=1.13.1, onnx=1.5.0, opset=10, tfonnx=1.5.0/82f805\n"
     ]
    }
   ],
   "source": [
    "import onnxmltools\n",
    "onnx_model = onnxmltools.convert_keras(keras_network)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (audio-experiments)",
   "language": "python",
   "name": "pycharm-1bd42f59"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
