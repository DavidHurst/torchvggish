{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Convert Tensorflow weights to PyTorch weights and save model\n",
    "# Download VGGish \n",
    "# Download the audioset directory using subversion\n",
    "# !apt-get -qq install subversion\n",
    "#!svn checkout https://github.com/tensorflow/models/trunk/research/audioset\n",
    "\n",
    "# Download audioset requirements\n",
    "#!pip install numpy scipy\n",
    "#!pip install resampy tensorflow six soundfile\n",
    "\n",
    "# grab the VGGish model checkpoints & PCA params\n",
    "#!curl -O https://storage.googleapis.com/audioset/vggish_model.ckpt\n",
    "#!curl -O https://storage.googleapis.com/audioset/vggish_pca_params.npz\n",
    "\n",
    "# Test install\n",
    "#!mv audioset/* .\n",
    "# from vggish_smoke_test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /Users/harrisontaylor/.conda/envs/audio_experiments/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/harrisontaylor/.conda/envs/audio_experiments/lib/python3.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /Users/harrisontaylor/.conda/envs/audio_experiments/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "vggish/conv1/weights:0\n",
      "\t(3, 3, 1, 64)\n",
      "vggish/conv1/biases:0\n",
      "\t(64,)\n",
      "vggish/conv2/weights:0\n",
      "\t(3, 3, 64, 128)\n",
      "vggish/conv2/biases:0\n",
      "\t(128,)\n",
      "vggish/conv3/conv3_1/weights:0\n",
      "\t(3, 3, 128, 256)\n",
      "vggish/conv3/conv3_1/biases:0\n",
      "\t(256,)\n",
      "vggish/conv3/conv3_2/weights:0\n",
      "\t(3, 3, 256, 256)\n",
      "vggish/conv3/conv3_2/biases:0\n",
      "\t(256,)\n",
      "vggish/conv4/conv4_1/weights:0\n",
      "\t(3, 3, 256, 512)\n",
      "vggish/conv4/conv4_1/biases:0\n",
      "\t(512,)\n",
      "vggish/conv4/conv4_2/weights:0\n",
      "\t(3, 3, 512, 512)\n",
      "vggish/conv4/conv4_2/biases:0\n",
      "\t(512,)\n",
      "vggish/fc1/fc1_1/weights:0\n",
      "\t(12288, 4096)\n",
      "vggish/fc1/fc1_1/biases:0\n",
      "\t(4096,)\n",
      "vggish/fc1/fc1_2/weights:0\n",
      "\t(4096, 4096)\n",
      "vggish/fc1/fc1_2/biases:0\n",
      "\t(4096,)\n",
      "vggish/fc2/weights:0\n",
      "\t(4096, 128)\n",
      "vggish/fc2/biases:0\n",
      "\t(128,)\n",
      "values written to vggish_dict\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import vggish_slim\n",
    "\n",
    "vggish_dict = {}\n",
    "# load the model and get info \n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    vggish_slim.define_vggish_slim(training=True)\n",
    "    vggish_slim.load_vggish_slim_checkpoint(sess,\"vggish_model.ckpt\")\n",
    "    \n",
    "    tvars = tf.trainable_variables()\n",
    "    tvars_vals = sess.run(tvars)\n",
    "\n",
    "    for var, val in zip(tvars, tvars_vals):\n",
    "#         print(var.name, val)  # Prints the name of the variable alongside its value.\n",
    "        print(\"%s\" % (var.name))\n",
    "        print(\"\\t\" + str(var.shape))\n",
    "        vggish_dict[var.name] = val\n",
    "    print(\"values written to vggish_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisontaylor/.conda/envs/audio_experiments/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type VGGish. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvggish\n",
    "# Define torch model for vggish\n",
    "# From vggish_slim:\n",
    "# The VGG stack of alternating convolutions and max-pools.\n",
    "#     net = slim.conv2d(net, 64, scope='conv1')\n",
    "#     net = slim.max_pool2d(net, scope='pool1')\n",
    "#     net = slim.conv2d(net, 128, scope='conv2')\n",
    "#     net = slim.max_pool2d(net, scope='pool2')\n",
    "#     net = slim.repeat(net, 2, slim.conv2d, 256, scope='conv3')\n",
    "#     net = slim.max_pool2d(net, scope='pool3')\n",
    "#     net = slim.repeat(net, 2, slim.conv2d, 512, scope='conv4')\n",
    "#     net = slim.max_pool2d(net, scope='pool4')\n",
    "#     # Flatten before entering fully-connected layers\n",
    "#     net = slim.flatten(net)\n",
    "#     net = slim.repeat(net, 2, slim.fully_connected, 4096, scope='fc1')\n",
    "#     # The embedding layer.\n",
    "#     net = slim.fully_connected(net, params.EMBEDDING_SIZE, scope='fc2')\n",
    "\n",
    "vggish_list = list(vggish_dict.values())\n",
    "def param_generator(conv=False, bias=False):\n",
    "    param = vggish_list.pop(0)\n",
    "    if conv:\n",
    "        transposed = np.transpose(param, (3, 2, 0, 1))\n",
    "    else:\n",
    "        transposed = np.transpose(param) # bias gets ignored this way\n",
    "    to_torch = torch.from_numpy(transposed)\n",
    "    result = torch.nn.Parameter(to_torch)\n",
    "    yield result\n",
    "\n",
    "class VGGish(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(VGGish, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(  1,  64, kernel_size=3, padding=1), # [batch_size, 64, 64, 96]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),         # [batch_size, 64, 32, 48]  \n",
    "            nn.Conv2d( 64, 128, kernel_size=3, padding=1), # [batch_size, 128, 32, 48]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),         # [batch_size, 128, 16, 24]\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1), # [batch_size, 256, 16, 24]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1), # [batch_size, 256, 16, 24]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),         # [batch_size, 256, 8, 12]\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1), # [batch_size, 512, 8, 12]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1), # [batch_size, 512, 8, 12]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)          # [batch_size, 512, 4, 6] \n",
    "        )\n",
    "        self.embeddings = nn.Sequential(\n",
    "            nn.Linear(512*4*6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 128),\n",
    "            nn.ReLU(inplace=True))\n",
    "        # extract weights from `vggish_list`\n",
    "        for seq in (self.features, self.embeddings):\n",
    "            for layer in seq:\n",
    "                if type(layer).__name__ != \"MaxPool2d\" and type(layer).__name__ != \"ReLU\":\n",
    "                    if type(layer).__name__ == \"Conv2d\":\n",
    "                        layer.weight = next(param_generator(conv=True))\n",
    "                    else:\n",
    "                        layer.weight = next(param_generator())\n",
    "                    layer.bias = next(param_generator())\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.embeddings(x)\n",
    "        return x\n",
    "\n",
    "net = VGGish()\n",
    "net.eval()\n",
    "\n",
    "# Save model to disk\n",
    "torch.save(net, \"./vggish-model.pth\")\n",
    "\n",
    "# Save weights\n",
    "model = torchvggish.VGGish()\n",
    "model.load_state_dict(net.state_dict(), strict=False)\n",
    "torch.save(model.state_dict(), \"./vggish-weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "824920086b9ba86f29fcc97607d44a0a409f001e950b4e2ab5e43a5169ea9415  ./vggish-weights.pth\r\n"
     ]
    }
   ],
   "source": [
    "# find the first 8 digits to rename vggish.pth to \n",
    "!shasum -a 256 ./vggish-weights.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a6c8d3a4c349319bbb8f7db886f033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "\n",
      "Mean Distance of embeddings (0=identical, -1=orthogonal): -0.41687705060396363\n",
      "Min distance of embeddings: -0.9207252786073556\n"
     ]
    }
   ],
   "source": [
    "# comparison\n",
    "import os\n",
    "import random\n",
    "import gzip\n",
    "import vggish_input\n",
    "import vggish_postprocess\n",
    "import vggish_params\n",
    "from tqdm import tqdm_notebook\n",
    "import scipy as sp\n",
    "\n",
    "distances = []\n",
    "for i in tqdm_notebook(range(5)):\n",
    "\n",
    "    def example_generator(base_path):\n",
    "        \"\"\"\n",
    "        Emits random examples from partition of audioset\n",
    "        \"\"\"\n",
    "        eval_files = os.listdir(base_path)\n",
    "        random.shuffle(eval_files)\n",
    "        while True:\n",
    "            eg_file = base_path + eval_files.pop(0)\n",
    "            with gzip.open(eg_file, 'rb') as wav_file:\n",
    "                yield vggish_input.wavfile_to_examples(wav_file)\n",
    "\n",
    "    eval_path = \"../../data/audioset_eval/\"\n",
    "    data_generator = example_generator(eval_path)\n",
    "    pproc = vggish_postprocess.Postprocessor(\"vggish_pca_params.npz\")\n",
    "    examples_batch = next(data_generator)\n",
    "\n",
    "    # Create an embedding with tensorflow vggish\n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "        vggish_slim.define_vggish_slim(training=False)\n",
    "        vggish_slim.load_vggish_slim_checkpoint(sess, \"vggish_model.ckpt\")\n",
    "        features_tensor = sess.graph.get_tensor_by_name(\n",
    "            vggish_params.INPUT_TENSOR_NAME)\n",
    "        embedding_tensor = sess.graph.get_tensor_by_name(\n",
    "            vggish_params.OUTPUT_TENSOR_NAME)\n",
    "\n",
    "        # Run inference and postprocessing.\n",
    "        [tf_embedding_batch] = sess.run([embedding_tensor],\n",
    "                                     feed_dict={features_tensor: examples_batch})\n",
    "        tf_postprocessed_batch = pproc.postprocess(tf_embedding_batch)\n",
    "    #     print(tf_postprocessed_batch)\n",
    "\n",
    "\n",
    "    #Create an embedding with pytorch vggish\n",
    "    pt_examples_batch = examples_batch[:,None,:,:]\n",
    "\n",
    "    # print(examples_batch.shape)\n",
    "    # pt_examples_batch = np.transpose(pt_examples_batch, (0, 3, 1, 2))\n",
    "    pt_examples_batch = torch.from_numpy(pt_examples_batch).float()\n",
    "    pt_embedding_batch = net.forward(pt_examples_batch)\n",
    "    pt_embedding_batch = pt_embedding_batch.data.numpy()\n",
    "    pt_postprocessed_batch = pproc.postprocess(pt_embedding_batch)\n",
    "\n",
    "    # print(\"PyTorch: {}\".format(pt_postprocessed_batch[0]))\n",
    "    # print(\"TensorF: {}\".format(tf_postprocessed_batch[0]))\n",
    "    for i in range(len(pt_postprocessed_batch)):\n",
    "        distances.append(sp.spatial.distance.cosine(pt_postprocessed_batch[i], \n",
    "                                                    tf_postprocessed_batch[i]))\n",
    "print(\"Mean Distance of embeddings (0=identical, -1=orthogonal): {}\".format(np.mean(distances)))\n",
    "print(\"Min distance of embeddings: {}\".format(np.min(distances)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (audio-experiments)",
   "language": "python",
   "name": "pycharm-1bd42f59"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
