{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D, GlobalMaxPooling2D\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras import backend as K\n",
    "\n",
    "import vggish_params\n",
    "import torchvggish \n",
    "\n",
    "import vggish_postprocess\n",
    "import scipy as sp\n",
    "\n",
    "\n",
    "# from : https://discuss.pytorch.org/t/transferring-weights-from-keras-to-pytorch/9889/2\n",
    "def keras_to_pyt(km, pm):\n",
    "    weight_dict = dict()\n",
    "    for layer in km.layers:\n",
    "        if type(layer) is keras.layers.convolutional.Conv2D:\n",
    "            weight_dict[layer.get_config()['name'] + '.weight'] = np.transpose(layer.get_weights()[0], (3, 2, 0, 1))\n",
    "            weight_dict[layer.get_config()['name'] + '.bias'] = layer.get_weights()[1]\n",
    "        elif type(layer) is keras.layers.Dense:\n",
    "            weight_dict[layer.get_config()['name'] + '.weight'] = np.transpose(layer.get_weights()[0], (1, 0))\n",
    "            weight_dict[layer.get_config()['name'] + '.bias'] = layer.get_weights()[1]\n",
    "    pyt_state_dict = pm.state_dict()\n",
    "    for key in pyt_state_dict.keys():\n",
    "        pyt_state_dict[key] = torch.from_numpy(weight_dict[key])\n",
    "    pm.load_state_dict(pyt_state_dict)\n",
    "    return pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ba9f7065db4edaa4b947093688df03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 96, 64)\n",
      "(1, 96, 64, 1)\n",
      "\n",
      "Mean Distance of embeddings (0=identical, -1=orthogonal): -0.2904446850380722\n",
      "Min distance of embeddings: -0.2904446850380722\n",
      "68d9c2db\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define nets\n",
    "\n",
    "# weight path\n",
    "# download from here: https://drive.google.com/open?id=1mhqXZ8CANgHyepum7N4yrjiyIg6qaMe6\n",
    "# (https://github.com/DTaoo/VGGish)\n",
    "WEIGHTS_PATH = './keras_weights.h5'\n",
    "\n",
    "def KerasVGGish(load_weights=True, weights='audioset',\n",
    "           input_tensor=None, input_shape=None,\n",
    "           out_dim=None, pooling='avg'):\n",
    "    '''\n",
    "    An implementation of the VGGish architecture.\n",
    "    :param load_weights: if load weights\n",
    "    :param weights: loads weights pre-trained on a preliminary version of YouTube-8M.\n",
    "    :param input_tensor: input_layer\n",
    "    :param input_shape: input data shape\n",
    "    :param out_dim: output dimension\n",
    "    :param include_top:whether to include the 3 fully-connected layers at the top of the network.\n",
    "    :param pooling: pooling type over the non-top network, 'avg' or 'max'\n",
    "    :return: A Keras model instance.\n",
    "    '''\n",
    "\n",
    "    if weights not in {'audioset', None}:\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization) or `audioset` '\n",
    "                         '(pre-training on audioset).')\n",
    "\n",
    "    if out_dim is None:\n",
    "        out_dim = vggish_params.EMBEDDING_SIZE\n",
    "\n",
    "    # input shape\n",
    "    if input_shape is None:\n",
    "        input_shape = (vggish_params.NUM_FRAMES, vggish_params.NUM_BANDS, 1)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        aud_input = Input(shape=input_shape, name='input_1')\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            aud_input = Input(tensor=input_tensor, shape=input_shape, name='input_1')\n",
    "        else:\n",
    "            aud_input = input_tensor\n",
    "\n",
    "\n",
    "    # Block 1\n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same', name='features.0')(aud_input)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='features.2')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same', name='features.3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='features.5')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='same', name='features.6')(x)\n",
    "    x = Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='same', name='features.8')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='features.10')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), strides=(1, 1), activation='relu', padding='same', name='features.11')(x)\n",
    "    x = Conv2D(512, (3, 3), strides=(1, 1), activation='relu', padding='same', name='features.13')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='features.15')(x)\n",
    "    # \n",
    "    x = Flatten(name='flatten_')(x) \n",
    "    x = Dense(4096, activation='relu', name='embeddings.0')(x)\n",
    "    x = Dense(4096, activation='relu', name='embeddings.2')(x)\n",
    "    x = Dense(out_dim, activation='relu', name='embeddings.4')(x)\n",
    "\n",
    "\n",
    "    inputs = aud_input\n",
    "    # Create model\n",
    "    model = Model(inputs, x, name='VGGish')\n",
    "    model.load_weights(WEIGHTS_PATH)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "class VGGish(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(VGGish, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(  1,  64, kernel_size=3, padding=1), # [batch_size, 64, 64, 96]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),         # [batch_size, 64, 32, 48]  \n",
    "            nn.Conv2d( 64, 128, kernel_size=3, padding=1), # [batch_size, 128, 32, 48]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),         # [batch_size, 128, 16, 24]\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1), # [batch_size, 256, 16, 24]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1), # [batch_size, 256, 16, 24]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),         # [batch_size, 256, 8, 12]\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1), # [batch_size, 512, 8, 12]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1), # [batch_size, 512, 8, 12]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)          # [batch_size, 512, 4, 6] \n",
    "        )\n",
    "        self.embeddings = nn.Sequential(\n",
    "            nn.Linear(512*4*6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 128),\n",
    "            nn.ReLU(inplace=True))\n",
    "                    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.embeddings(x)\n",
    "        return x\n",
    "    \n",
    "def TorchVGGish():\n",
    "    net = VGGish()\n",
    "    return net\n",
    "\n",
    "\n",
    "\n",
    "keras_model = KerasVGGish()\n",
    "pyt_model = TorchVGGish().eval()\n",
    "pyt_model = keras_to_pyt(keras_model, pyt_model)\n",
    "\n",
    "pproc = vggish_postprocess.Postprocessor(\"vggish_pca_params.npz\")\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "distances = []\n",
    "for i in tqdm_notebook(range(1)):\n",
    "    inp = np.random.normal(size=(1, 1, 96, 64)).astype(dtype=np.float32)\n",
    "    print(inp.shape)\n",
    "    inp_pyt = torch.autograd.Variable(torch.from_numpy(inp.copy()).float())\n",
    "    inp_keras = np.transpose(inp.copy(), (0, 2, 3, 1))\n",
    "    print(inp_keras.shape)\n",
    "    \n",
    "    keras_result = keras_model.predict(x=inp_keras, verbose=0)\n",
    "    pyt_res = pyt_model(inp_pyt).data.numpy()\n",
    "    \n",
    "    embedding_pt = pproc.postprocess(pyt_res)\n",
    "    embedding_keras = pproc.postprocess(keras_result)\n",
    "    \n",
    "    distance = sp.spatial.distance.cosine(embedding_keras,embedding_pt)\n",
    "    distances.append(distance)\n",
    "print(\"Mean Distance of embeddings (0=identical, -1=orthogonal): {}\".format(np.mean(distances)))\n",
    "print(\"Min distance of embeddings: {}\".format(np.min(distances)))\n",
    "\n",
    "plt.hist(distances, bins=50)\n",
    "plt.plot()\n",
    "\n",
    "model = torchvggish.VGGish()\n",
    "model.load_state_dict(pyt_model.state_dict(), strict=False)\n",
    "torch.save(model.state_dict(), \"./vggish-weights.pth\")\n",
    "# Get first 8 chars of sha 256 hash for uploading \n",
    "! shasum -a 256 ./vggish-weights.pth | awk '{print substr($0,0,8)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b248c6ee5b449069e0dfdceafa3e524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "\n",
      "True\n",
      "Mean Distance of pt embeddings (0=identical, -1=orthogonal): -0.2904446850380722\n",
      "Min distance of pt embeddings: -0.2904446850380722\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# comparison\n",
    "import os\n",
    "import random\n",
    "import gzip\n",
    "import vggish_input\n",
    "import vggish_postprocess\n",
    "import vggish_params\n",
    "import vggish_slim\n",
    "from tqdm import tqdm_notebook\n",
    "import scipy as sp\n",
    "\n",
    "pt_distances = []\n",
    "for i in tqdm_notebook(range(100)):\n",
    "\n",
    "    def example_generator(base_path):\n",
    "        \"\"\"\n",
    "        Emits random examples from partition of audioset\n",
    "        \"\"\"\n",
    "        eval_files = os.listdir(base_path)\n",
    "        random.shuffle(eval_files)\n",
    "        while True:\n",
    "            eg_file = base_path + eval_files.pop(0)\n",
    "            with gzip.open(eg_file, 'rb') as wav_file:\n",
    "                yield vggish_input.wavfile_to_examples(wav_file)[0:1]\n",
    "\n",
    "    eval_path = \"../../data/audioset_eval/\"\n",
    "    data_generator = example_generator(eval_path)\n",
    "    pproc = vggish_postprocess.Postprocessor(\"vggish_pca_params.npz\")\n",
    "    examples_batch = next(data_generator)\n",
    "\n",
    "    # Create an embedding with tensorflow vggish\n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "        vggish_slim.define_vggish_slim(training=False)\n",
    "        vggish_slim.load_vggish_slim_checkpoint(sess, \"vggish_model.ckpt\")\n",
    "        features_tensor = sess.graph.get_tensor_by_name(\n",
    "            vggish_params.INPUT_TENSOR_NAME)\n",
    "        embedding_tensor = sess.graph.get_tensor_by_name(\n",
    "            vggish_params.OUTPUT_TENSOR_NAME)\n",
    "\n",
    "        # Run inference and postprocessing.\n",
    "        [tf_embedding_batch] = sess.run([embedding_tensor],\n",
    "                                     feed_dict={features_tensor: examples_batch})\n",
    "        tf_postprocessed_batch = pproc.postprocess(tf_embedding_batch)\n",
    "    \n",
    "    # Create an embedding with keras\n",
    "    inp_keras = examples_batch[:,:,:,None].astype(dtype=np.float32)\n",
    "    k_embedding_batch = keras_model.predict(inp_keras)\n",
    "    k_postprocessed_batch = pproc.postprocess(k_embedding_batch)\n",
    "    \n",
    "    # Create an embedding with pytorch\n",
    "    inp_pyt = np.transpose(inp_keras.copy(), (0, 3, 1, 2))\n",
    "    inp_pyt = torch.autograd.Variable(torch.from_numpy(inp_pyt).float())\n",
    "    pt_embedding_batch = pyt_model(inp_pyt).data.numpy()\n",
    "    pt_postprocessed_batch = pproc.postprocess(pt_embedding_batch)\n",
    "    \n",
    "    \n",
    "  \n",
    "    #Create an embedding with pytorch vggish\n",
    "#     pt_examples_batch = examples_batch.copy()[:,None,:,:]\n",
    "    # print(examples_batch.shape)\n",
    "    # pt_examples_batch = np.transpose(pt_examples_batch, (0, 3, 1, 2))\n",
    "#     pt_examples_batch = torch.from_numpy(pt_examples_batch).float()\n",
    "#     pt_embedding_batch = model(pt_examples_batch)\n",
    "#     pt_embedding_batch = pt_embedding_batch.data.numpy()\n",
    "    \n",
    "\n",
    "    # print(\"PyTorch: {}\".format(pt_postprocessed_batch[0]))\n",
    "    # print(\"TensorF: {}\".format(tf_postprocessed_batch[0]))\n",
    "    for i in range(len(pt_postprocessed_batch)):\n",
    "        pt_distances.append(sp.spatial.distance.cosine(pt_postprocessed_batch[i], \n",
    "                                                    k_postprocessed_batch[i]))\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "print(all(True for elem in (k_postprocessed_batch == tf_postprocessed_batch)))\n",
    "print(\"Mean Distance of pt embeddings (0=identical, -1=orthogonal): {}\".format(np.mean(distances)))\n",
    "print(\"Min distance of pt embeddings: {}\".format(np.min(distances)))\n",
    "\n",
    "fig=plt.figure(figsize=(16,8))\n",
    "fig.add_subplot(1,3,1)\n",
    "plt.imshow(np.reshape(pt_postprocessed_batch[0],(16,8)))\n",
    "plt.title(\"PyTorch\")\n",
    "fig.add_subplot(1,3,2)\n",
    "plt.imshow(np.reshape(tf_postprocessed_batch[0],(16,8)))\n",
    "plt.title(\"TensorFlow\")\n",
    "fig.add_subplot(1,3,3)\n",
    "plt.imshow(np.reshape(k_postprocessed_batch[0],(16,8)))\n",
    "plt.title(\"Keras\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Distance of pt embeddings (0=identical, -1=orthogonal): -0.3834115944823616\n",
      "Min distance of pt embeddings: -0.7432834237019283\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Distance of pt embeddings (0=identical, -1=orthogonal): {}\".format(np.mean(pt_distances)))\n",
    "print(\"Min distance of pt embeddings: {}\".format(np.min(pt_distances)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12288])\n",
      "(1, 128)\n",
      "Cumulative distance (5.d.p): -1.9999220967292786e-05\n"
     ]
    }
   ],
   "source": [
    "WEIGHTS_PATH = './keras_weights.h5'\n",
    "# tf.set_random_seed(42)\n",
    "# random.seed(41)\n",
    "# np.random.seed(42)\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "def keras_to_pyt(km, pm):\n",
    "    weight_dict = dict()\n",
    "    for layer in km.layers:\n",
    "        if type(layer) is keras.layers.convolutional.Conv2D:\n",
    "            weight_dict[layer.get_config()['name'] + '.weight'] = np.transpose(layer.get_weights()[0], (3, 2, 0, 1))\n",
    "            weight_dict[layer.get_config()['name'] + '.bias'] = layer.get_weights()[1]\n",
    "        elif type(layer) is keras.layers.Dense:\n",
    "            weight_dict[layer.get_config()['name'] + '.weight'] = np.transpose(layer.get_weights()[0], (1, 0))\n",
    "            weight_dict[layer.get_config()['name'] + '.bias'] = layer.get_weights()[1]\n",
    "    pyt_state_dict = pm.state_dict()\n",
    "    for key in pyt_state_dict.keys():\n",
    "        pyt_state_dict[key] = torch.from_numpy(weight_dict[key])\n",
    "    pm.load_state_dict(pyt_state_dict)\n",
    "    return pm\n",
    "\n",
    "def KerasVGGish(load_weights=True, weights='audioset',\n",
    "           input_tensor=None, input_shape=None,\n",
    "           out_dim=None, pooling='avg'):\n",
    "    '''\n",
    "    An implementation of the VGGish architecture.\n",
    "    :param load_weights: if load weights\n",
    "    :param weights: loads weights pre-trained on a preliminary version of YouTube-8M.\n",
    "    :param input_tensor: input_layer\n",
    "    :param input_shape: input data shape\n",
    "    :param out_dim: output dimension\n",
    "    :param include_top:whether to include the 3 fully-connected layers at the top of the network.\n",
    "    :param pooling: pooling type over the non-top network, 'avg' or 'max'\n",
    "    :return: A Keras model instance.\n",
    "    '''\n",
    "\n",
    "    if weights not in {'audioset', None}:\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization) or `audioset` '\n",
    "                         '(pre-training on audioset).')\n",
    "\n",
    "    if out_dim is None:\n",
    "        out_dim = vggish_params.EMBEDDING_SIZE\n",
    "\n",
    "    # input shape\n",
    "    if input_shape is None:\n",
    "        input_shape = (vggish_params.NUM_FRAMES, vggish_params.NUM_BANDS, 1)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        aud_input = Input(shape=input_shape, name='input_1')\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            aud_input = Input(tensor=input_tensor, shape=input_shape, name='input_1')\n",
    "        else:\n",
    "            aud_input = input_tensor\n",
    "\n",
    "\n",
    "    # Block 1\n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same', name='features.0')(aud_input)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='features.2')(x)\n",
    "\n",
    "#     # Block 2\n",
    "    x = Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same', name='features.3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='features.5')(x)\n",
    "\n",
    "#     # Block 3\n",
    "    x = Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='same', name='features.6')(x)\n",
    "    x = Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='same', name='features.8')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='features.10')(x)\n",
    "\n",
    "#     # Block 4\n",
    "    x = Conv2D(512, (3, 3), strides=(1, 1), activation='relu', padding='same', name='features.11')(x)\n",
    "    x = Conv2D(512, (3, 3), strides=(1, 1), activation='relu', padding='same', name='features.13')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='features.15')(x)\n",
    "#     # \n",
    "    x = Flatten(name='flatten_')(x) \n",
    "    x = Dense(4096, activation='relu', name='embeddings.0')(x)\n",
    "    x = Dense(4096, activation='relu', name='embeddings.2')(x)\n",
    "    x = Dense(out_dim, activation='relu', name='embeddings.4')(x)\n",
    "\n",
    "\n",
    "    inputs = aud_input\n",
    "    # Create model\n",
    "    model = Model(inputs, x, name='VGGish')\n",
    "#     model.load_weights(WEIGHTS_PATH)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "class VGGish(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(VGGish, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(  1,  64, kernel_size=3, padding=1), # [batch_size, 64, 64, 96]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),         # [batch_size, 64, 32, 48]  \n",
    "            nn.Conv2d( 64, 128, kernel_size=3, padding=1), # [batch_size, 128, 32, 48]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),         # [batch_size, 128, 16, 24]\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1), # [batch_size, 256, 16, 24]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1), # [batch_size, 256, 16, 24]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),         # [batch_size, 256, 8, 12]\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1), # [batch_size, 512, 8, 12]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1), # [batch_size, 512, 8, 12]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)          # [batch_size, 512, 4, 6] \n",
    "        )\n",
    "        self.embeddings = nn.Sequential(\n",
    "            nn.Linear(512*4*6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "                    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.transpose(x, 1, 3).contiguous()\n",
    "        x = torch.transpose(x, 1, 2).contiguous()\n",
    "        x = x.view(x.size(0),-1)\n",
    "        print(x.shape)\n",
    "        x = self.embeddings(x)\n",
    "        return x\n",
    "    \n",
    "def TorchVGGish():\n",
    "    net = VGGish()\n",
    "    return net\n",
    "\n",
    "\n",
    "\n",
    "keras_model = KerasVGGish()\n",
    "pyt_model = TorchVGGish().eval()\n",
    "pyt_model = keras_to_pyt(keras_model, pyt_model)\n",
    "\n",
    "base_path = \"../../data/audioset_eval/\"\n",
    "eval_files = os.listdir(base_path)\n",
    "eg_file = base_path + eval_files.pop(0)\n",
    "with gzip.open(eg_file, 'rb') as wav_file:\n",
    "    inp = vggish_input.wavfile_to_examples(wav_file)[0:1][None,:,:,:].astype(dtype=np.float32)\n",
    "\n",
    "# print(inp.shape)\n",
    "# inp = np.random.normal(size=(1, 1, 96, 64)).astype(dtype=np.float32)\n",
    "inp_pyt = torch.autograd.Variable(torch.from_numpy(inp.copy()).float())\n",
    "inp_keras = np.transpose(inp.copy(), (0, 2, 3, 1))\n",
    "\n",
    "keras_result = keras_model.predict(x=inp_keras, verbose=0)\n",
    "# features \n",
    "# pyt_result = np.transpose(pyt_model(inp_pyt).data.numpy(), (0, 2, 3, 1))\n",
    "pyt_result = pyt_model(inp_pyt).data.numpy()\n",
    "\n",
    "cumulative_distance = 0\n",
    "# Feature layers are ok! \n",
    "# for i in range(1):\n",
    "#     for j in range(6):\n",
    "#         for k in range(4):\n",
    "#             for l in range(512):\n",
    "# #                 print(keras_result.shape)\n",
    "# #                 print(pyt_result.shape)\n",
    "#                 local_distance = keras_result[i, j, k, l] - pyt_result[i, j, k, l]\n",
    "#                 cumulative_distance += local_distance\n",
    "# #                 print(keras_result[i, j, k, l], pyt_result[i, j, k, l])\n",
    "# #                 print(\"distance: {}\".format(local_distance))\n",
    "# print(cumulative_distance)\n",
    "\n",
    "# # flatten layer\n",
    "# for i in range(12288):\n",
    "#     cumulative_distance += (keras_result[0][i] - pyt_result[i])\n",
    "# print(cumulative_distance)\n",
    "\n",
    "print(keras_result.shape)\n",
    "\n",
    "# embedding layer \n",
    "for i in range(128):\n",
    "    cumulative_distance += (round(keras_result[0][i],5) - round(pyt_result[0][i],5))\n",
    "print(\"Cumulative distance (5.d.p): {}\".format( cumulative_distance))\n",
    "\n",
    "\n",
    "\n",
    "# embedding_pt = pproc.postprocess(pyt_res)\n",
    "# embedding_keras = pproc.postprocess(keras_result)\n",
    "\n",
    "# distance = sp.spatial.distance.cosine(keras_result, pyt_result)\n",
    "# print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d0995ffdbe424fb88e7413ef705fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [1 x 122880], m2: [12288 x 4096] at /Users/distiller/project/conda/conda-bld/pytorch_1556653464916/work/aten/src/TH/generic/THTensorMath.cpp:961",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-178-0553baa53a94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0minp_pyt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0minp_pyt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_pyt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mpt_embedding_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyt_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_pyt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mpt_postprocessed_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpt_embedding_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/audio_experiments/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-176-1272e59d510a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/audio_experiments/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/audio_experiments/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/audio_experiments/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/audio_experiments/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/audio_experiments/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1408\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [1 x 122880], m2: [12288 x 4096] at /Users/distiller/project/conda/conda-bld/pytorch_1556653464916/work/aten/src/TH/generic/THTensorMath.cpp:961"
     ]
    }
   ],
   "source": [
    "# comparison\n",
    "import os\n",
    "import random\n",
    "import gzip\n",
    "import vggish_input\n",
    "import vggish_postprocess\n",
    "import vggish_params\n",
    "import vggish_slim\n",
    "from tqdm import tqdm_notebook\n",
    "import scipy as sp\n",
    "\n",
    "pt_distances = []\n",
    "for i in tqdm_notebook(range(100)):\n",
    "\n",
    "    def example_generator(base_path):\n",
    "        \"\"\"\n",
    "        Emits random examples from partition of audioset\n",
    "        \"\"\"\n",
    "        eval_files = os.listdir(base_path)\n",
    "        random.shuffle(eval_files)\n",
    "        while True:\n",
    "            eg_file = base_path + eval_files.pop(0)\n",
    "            with gzip.open(eg_file, 'rb') as wav_file:\n",
    "                yield vggish_input.wavfile_to_examples(wav_file)\n",
    "\n",
    "    eval_path = \"../../data/audioset_eval/\"\n",
    "    data_generator = example_generator(eval_path)\n",
    "    pproc = vggish_postprocess.Postprocessor(\"vggish_pca_params.npz\")\n",
    "    examples_batch = next(data_generator)\n",
    "\n",
    "    # Create an embedding with tensorflow vggish\n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "        vggish_slim.define_vggish_slim(training=False)\n",
    "        vggish_slim.load_vggish_slim_checkpoint(sess, \"vggish_model.ckpt\")\n",
    "        features_tensor = sess.graph.get_tensor_by_name(\n",
    "            vggish_params.INPUT_TENSOR_NAME)\n",
    "        embedding_tensor = sess.graph.get_tensor_by_name(\n",
    "            vggish_params.OUTPUT_TENSOR_NAME)\n",
    "\n",
    "        # Run inference and postprocessing.\n",
    "        [tf_embedding_batch] = sess.run([embedding_tensor],\n",
    "                                     feed_dict={features_tensor: examples_batch})\n",
    "        tf_postprocessed_batch = pproc.postprocess(tf_embedding_batch)\n",
    "    \n",
    "    # Create an embedding with keras\n",
    "    inp_keras = examples_batch[:,:,:,None].astype(dtype=np.float32)\n",
    "    k_embedding_batch = keras_model.predict(inp_keras)\n",
    "    k_postprocessed_batch = pproc.postprocess(k_embedding_batch)\n",
    "    \n",
    "    # Create an embedding with pytorch\n",
    "    inp_pyt = np.transpose(inp_keras.copy(), (0, 3, 1, 2))\n",
    "    inp_pyt = torch.autograd.Variable(torch.from_numpy(inp_pyt).float())\n",
    "    pt_embedding_batch = pyt_model(inp_pyt).data.numpy()\n",
    "    pt_postprocessed_batch = pproc.postprocess(pt_embedding_batch)\n",
    "    \n",
    "    \n",
    "  \n",
    "    #Create an embedding with pytorch vggish\n",
    "#     pt_examples_batch = examples_batch.copy()[:,None,:,:]\n",
    "    # print(examples_batch.shape)\n",
    "    # pt_examples_batch = np.transpose(pt_examples_batch, (0, 3, 1, 2))\n",
    "#     pt_examples_batch = torch.from_numpy(pt_examples_batch).float()\n",
    "#     pt_embedding_batch = model(pt_examples_batch)\n",
    "#     pt_embedding_batch = pt_embedding_batch.data.numpy()\n",
    "    \n",
    "\n",
    "    # print(\"PyTorch: {}\".format(pt_postprocessed_batch[0]))\n",
    "    # print(\"TensorF: {}\".format(tf_postprocessed_batch[0]))\n",
    "    for i in range(len(pt_postprocessed_batch)):\n",
    "        pt_distances.append(sp.spatial.distance.cosine(pt_postprocessed_batch[i], \n",
    "                                                    k_postprocessed_batch[i]))\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "print(all(True for elem in (k_postprocessed_batch == tf_postprocessed_batch)))\n",
    "print(\"Mean Distance of pt embeddings (0=identical, -1=orthogonal): {}\".format(np.mean(distances)))\n",
    "print(\"Min distance of pt embeddings: {}\".format(np.min(distances)))\n",
    "\n",
    "fig=plt.figure(figsize=(16,8))\n",
    "fig.add_subplot(1,3,1)\n",
    "plt.imshow(np.reshape(pt_postprocessed_batch[0],(16,8)))\n",
    "plt.title(\"PyTorch\")\n",
    "fig.add_subplot(1,3,2)\n",
    "plt.imshow(np.reshape(tf_postprocessed_batch[0],(16,8)))\n",
    "plt.title(\"TensorFlow\")\n",
    "fig.add_subplot(1,3,3)\n",
    "plt.imshow(np.reshape(k_postprocessed_batch[0],(16,8)))\n",
    "plt.title(\"Keras\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (audio-experiments)",
   "language": "python",
   "name": "pycharm-1bd42f59"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
